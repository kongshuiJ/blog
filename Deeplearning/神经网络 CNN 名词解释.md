## 隐藏层
不是输入或输出层的所有层都称为隐藏层.

## 激活和池化都没有权重
使层与操作区分开的原因在于层具有权重。由于池操作和激活功能没有权重，因此我们将它们称为操作，并将其视为已添加到层操作集合中。
例如，我们说网络中的第二层是一个卷积层，其中包含权重的集合，并执行三个操作，即卷积操作，relu激活操作和最大池化操作。

## 传入Linear层之前展平张量
在将输入传递到第一个隐藏的Linear层之前，我们必须reshape()或展平我们的张量。每当我们将卷积层的输出作为Linear层的输入传递时，都是这种情况。

## 正向传播
正向传播是将输入张量转换为输出张量的过程。神经网络的核心是将输入张量映射到输出张量的功能，而正向传播只是将传递输入到网络并从网络接收输出的过程的特殊名称。

## in_channels
对于最初输入图片样本的通道数in_channels取决于图片的类型，如果是彩色的，即RGB类型，这时候通道数固定为3，如果是灰色的，通道数为1。

## out_channels
卷积完成之后，输出的通道数out_channels取决于过滤器的数量。从这个方向理解，这里的out_channels设置的就是过滤器的数目。
对于第二层或者更多层的卷积，此时的 in_channels 就是上一层的 out_channels ， out_channels 还是取决于过滤器数目。

## 为什么GPU在深度学习中能够如此广泛的使用？
因为神经网络是易并行的(embarrassing parallel),即：很容易就能够将任务分解成一组独立的小任务；神经网络的很多计算都可以很容易地分解成更小的相互独立的计算，这使得GPU在深度学习任务中非常有用。
一个卷积核对一张图像进行卷积的每个运算是独立且相继发生的，故可将其分成一个个小任务，使用GPU加速运算； (图像分块后送入神经网络是否同理？)

## 张量
- 张量是神经网络中使用的主要数据结构，网络中的输入、输出和转换均使用张量表示.
- 张量与张量之间的运算必须是相同数据类型在相同的设备上发生的.
- 张量是包含一个同一类型的数据.

| 索引数量 | 计算机科学中的名称	 | 数学中的名称 | Tensor表示 |
|   ---    | ---                 | ---          | ---        |
| 0        | 数字                | 标量         | 0维张量    |
| 1        | 数组                | 矢量         | 1维张量    |
| 2        | 二维数组            | 矩阵         | 2维张量    |
| n        | N维数组             | N维张量      | n维张量    |

### 张量的属性(阶,轴,形状)
#### 阶
- 张量的阶是指张量中的维数。假设我们有一个二阶张量。这意味着以下所有内容：
- 一个张量的秩告诉我们需要多少个索引来访问或引用张量结构中包含的特定数据元素

#### 轴
- 一个张量的轴是一个张量的一个特定维度。
- 对于张量，其最后一个轴的元素均为数字。
- 假设有个张量是一个2阶的张量，这意味着这个张量有2个维度，或者等价于，张量有 2 个轴。

#### 形状
- 张量的形状由每个轴的长度决定（知道了张量的形状就可知道每个轴的索引）。
- 3 x 3的形状告诉我们，这个2阶张量的每个轴的长度都是3，这意味着我们有三个沿着每个轴可用的索引。

### 张量输入到神经网络
- CNN输入张量的长度通常为4：[batchsize,color_channel,height,width]；通过这4个索引，可以在特定图像的特定颜色通道中导航到特定的像素；
- 卷积神经网络的样本输入通常是批量的而不是单个的；
- 张量经过卷积层后的变化：卷积会改变高度、宽度以及颜色通道的数量；通道数与滤波器的数量有关；滤波器的大小会影响到高度和宽度；
- 经过卷积的通道不再叫彩色通道(已被改变)，而叫做特征通道(特征图:输入颜色通道和卷积滤波器所产生的卷积结果)
- 输出通道 = 特征通道 = 特征映射

### CNN构建及网络参数的使用
#### Parameter和Argument的区别
- Parameter在函数定义中使用，可将其看作是占位符；(形参)
- Argument是当函数被调用时传递给函数的实际值；（实参）
#### Parameter的两种类型
##### Hyperparameters
其值是手动和任意确定的；要构建神经网络：kernel_size,out_channels,out_features都需要手动选择.
##### Data dependent Hyperparameters
其值是依赖于数据的参数
- 该参数位于网络的开始或末端，即第一个卷积层的输入通道和最后一个卷积层的输出特征图
- 第一个卷积层的输入通道依赖于构成训练集的图像内部的彩色通道的数量（灰度图像是1，彩色图像是3）
- 输出层的输出特征依赖于训练集中类的数量（fashion-MNIST数据集中的类型为10，则输出层的out_features=10）
- 通常情况下，一层的输入是上一层的输出（即：卷积层中所有输入通道和线性层中的输入特征都依赖于上一层的数据）
#### 当张量从卷积层传入线性层时，张量必须是flatten的

| Parameter       | Description                                  |
| ---             | ---                                          |
| kernel_size     | 设置滤波器的大小；滤波器的数量就是输出通道数 |
| out_channels    | 设置滤波器的数量，即为输出通道数             |
| out_features    | 设置输出张量的大小                           |

## 训练神经网络的七个步骤
1. 从训练集中获取批量数据
2. 将批量数据传入网络
3. 计算损失(预测值与真实值之间的差)【需要loss function实现】
4. 计算损失函数的梯度 【需要back propagation实现】
5. 通过上一步计算的梯度来更新权重，进而减少损失【需要optimization algorithm实现】
6. 重复1-5步直到一个epoch执行完成
7. 重复1-6步直到所设定的epochs执行完成并得到满意的accuracy #### 3.12.2 单批次图像训练

## 每个周期的迭代数
数据总数/batchsize（当改变batchsize时，也就是改变了更新权重的次数，也就是朝损失函数最小的防线前进的步数）
 
## 梯度
告诉我们应该走哪条路能更快的到达loss最小